# FastAgent Configuration File

# Default Model Configuration:
# 
# Takes format:
#   <provider>.<model_string>.<reasoning_effort?> (e.g. anthropic.claude-3-5-sonnet-20241022 or openai.o3-mini.low)
# Accepts aliases for Anthropic Models: haiku, haiku3, sonnet, sonnet35, opus, opus3
# and OpenAI Models: gpt-4.1, gpt-4.1-mini, o1, o1-mini, o3-mini
#
# If not specified, defaults to "haiku". 
# Can be overriden with a command line switch --model=<model>, or within the Agent constructor.

default_model: openai.gpt-4.1-nano

# Logging and Console Configuration:
logger:
    # level: "debug" | "info" | "warning" | "error"
    level: "debug"
    # type: "none" | "console" | "file" | "http"
    type: "file"
    path: "logfile.jsonl"

    
    # Switch the progress display on or off
    progress_display: true

    # Show chat User/Assistant messages on the console
    show_chat: true
    # Show tool calls on the console
    show_tools: true
    # Truncate long tool responses on the console 
    truncate_tools: true

# MCP Servers
# database: "./examples/sqlite_servers/mcp_servers.db"

# qdrant_url: "http://192.168.194.33:6333"

mcp:
    servers:
        playwright:
            transport: "sse"
            url: "http://192.168.194.33:8111/sse"
            # command: "npx"
            # args: [ "@playwright/mcp@latest", "--headless"]

        searxng:
            command: "uvx"
            args: ["mcp-searxng"]
            env:
                SEARXNG_URL: "http://192.168.194.33:8060"
        # tectonic:
        #     command: "uv"
        #     args: ["--directory","/home/therm/src/tectonic-mcp-server","run","tectonic-mcp.py"]
        fetch:
            command: "uvx"
            args: ["mcp-server-fetch"]
        # everything:
        #     command: "npx"
        #     args: ["-y", "@modelcontextprotocol/server-everything"]
        # everything:
        #     transport: "streamable"
        #     url: "http://localhost:3001/mcp"
